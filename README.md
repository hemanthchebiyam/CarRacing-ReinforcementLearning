# OpenAI Gym Car Racing Environment

## Code written by: Hemanth Chebiyam, Sanjeev Vijayakumar

OpenAI Gym is an open source Python library for developing and comparing reinforcement learning algorithms by providing a standard API to communicate between learning algorithms and environments.

We aim to use the stable baseline 3 algorithms that allows to experiment on various Deep Reinforcement Learning Algorithms. Stable Baselines3 is a powerful reinforcement learning library that provides a collection of state-of-the-art algorithms for training and evaluating reinforcement learning agents. These algorithms are designed to be stable, efficient, and easy to use, making them suitable for a wide range of applications in research and industry. These algorithms offer robust performance across various environments, enabling researchers and practitioners to tackle complex reinforcement learning tasks effectively. With its intuitive API and extensive documentation, Stable Baselines3 serves as a valuable tool for testing in the car racing environment.

Further, to observe the car racing environments under different conditions, we use the PPO algorithm (Proximal Policy Optimization) and the DQN algorithm (Deep Q-Network) to help train in the environment. With an agent trained for over 500k episodes and 2 million episodes, relations can be drawn.

# PPO Algorithm Training Runs

## No Training
![](https://github.com/hemanthchebiyam/CarRacing-ReinforcementLearning/blob/main/Resources/NoTraining.gif)

## 500k steps Training
![](https://github.com/hemanthchebiyam/CarRacing-ReinforcementLearning/blob/main/Resources/500k_Training.gif)

## 2 mil steps Training
![](https://github.com/hemanthchebiyam/CarRacing-ReinforcementLearning/blob/main/Resources/2mil_Training.gif)
